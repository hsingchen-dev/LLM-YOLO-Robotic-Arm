Languages: [日本語](README_ja.md) | [English](README.md)

# LLM-YOLO-Robotic-Arm  
LLM（ChatGPT）とYOLOを活用した音声制御ロボットアームシステム

---

# はじめに（Introduction）  

私たちは、生成AIと日本の伝統的な自動化技術の融合を研究する学際的なチームです。  
本プロジェクトの目的は、ChatGPT、YOLO、逆運動学を組み合わせることで、ロボットアームが自然言語の命令を理解し、日本の高度な自動化システムと連携して適切な動作を実行できるようにすることです。  

本研究の主なテーマは以下のとおりです：  
- **自然言語処理（NLP）：** 大型言語モデル（LLM）による意味解析  
- **コンピュータビジョン（CV）：** YOLOによる物体認識  
- **ロボット制御：** 逆運動学によるロボットアームの動作計算  
- **ヒューマン・ロボット・インタラクション：** 音声入力と動作指示の最適化  

 
本プロジェクトは、成果の中核は神戸大学学生に帰属します。
本プロジェクトは、AIとロボティクスの融合による「次世代スマート制御」の実証モデルを目指しています。

---

# Agent-SKYNET  

Sky Netは、LLM（大型言語モデル）と現実世界のロボット制御をつなぐために私たちのチームが開発したモジュール型のソフトウェアエージェントです。  
自然言語と機械の動作の間における知能的インターフェースとして機能し、Sky NetはChatGPTを中核の推論エンジンとして使用しています。音声またはテキストコマンドを受け取ると、意味解析・文脈理解・意思決定を行い、物体認識モジュール（YOLO）および制御レイヤー（逆運動学・サーボ制御）と連携して、6自由度のロボットアームを介して適切な物理的動作を実行します。

このアーキテクチャにより、リアルタイムかつ直感的な人間とロボットのインタラクションが実現され、言語駆動による物体操作が可能になります。  
Sky Netは、「言語を理解するだけでなく、現実世界で意味のある行動をとるAI」すなわち**身体性を持つAI（Embodied AI）**への一歩となります。

---

# 概要（Overview）  

LLM-Robotic-Armは、YOLOによる物体認識とChatGPTによる自然言語理解を統合し、音声ガイドによる6自由度ロボットアームの制御を実現するシステムです。  
本システムは、リアルタイム物体認識、ソケット通信、逆運動学による正確な動作制御をサポートしています。  
本システムは、視覚・言語・動作の3要素を統合するマルチモーダルAIロボット制御の実例として注目されています。

---

# 特徴（Features）

- 音声ベースの物体操作  
- YOLOによるリアルタイム物体検出  
- ChatGPT（LLM）による自然言語理解  
- Raspberry Piベースのデプロイ  
- 逆運動学とサーボ制御対応  

---

# チーム（Team）

本プロジェクトは、博士課程および修士課程の学生による学際的チームによって構成されており、各メンバーが専門分野に基づいて役割を分担しています。

- **杭 星辰（ハン シンチェン）**（プロジェクトリーダー）  
  ロボットアーム制御、逆運動学、YOLO統合、LLMベースシステム設計を担当。  
  社会向けプレゼン経験、国際共同研究経験。  
  *神戸大学*

- **丸山 晴樹（マルヤマ ハルキ）**  
  エージェントによる提示詞処理と、日本語における対話スタイルの調整を担当。  
  自然な日本語応答の設計と文化的文脈の理解を支援。  
  *東京大学*

- **李 天洋（リ テンヨウ）**  
  3Dモデリング（CAD/SolidWorks）、ロボットの機械設計およびプロトタイピングを担当。  
  YOLOベースの把持と逆運動学の調整も担当。  
  *神戸大学*

- **孫 羽杉（ソン ユシャン）**  
  システム統合、ソケット通信、バックエンド開発を担当。  
  *神戸大学*

- **孫 妍（ソン ケン）**  
  人間行動とロボットインタラクションを専門とし、エージェントによる提示詞処理とシステムの行動ロジック設計を担当。  
  *神戸大学*

- **姜 啓龍（キョウ ケイリュウ）**  
  Streamlit UI設計およびUX体験の最適化を担当。  
  *デザインおよびソフトウェア工学の背景を持つ*



## 実演ビデオ SKYNET-5  
https://www.youtube.com/watch?v=69e78PqmeNM&t=3s  
本バージョンでは、システムが簡単な言語指令を理解し、それに応じた物理的動作を実行できます。

## 実演ビデオ SKYNET-6  
https://www.youtube.com/watch?v=lS7rUFcXonQ  
このバージョンでは、LLMが人間の行動、認知、心理を学習し、人間らしい意識の要素をシミュレーションします。日本語の敬語理解も含まれています。

## 実演ビデオ SKYNET-6.1  
https://www.youtube.com/watch?v=jr8Sl4M8Fsw  
このバージョンでは、音声制御が追加されています。

## 実演ビデオ SKYNET-8  
https://www.youtube.com/watch?v=Eo-8q8rrNC4  
YOLOベースの物体検出と逆運動学が統合され、単純な物体を認識し、把持動作を実行できます。

## 実演ビデオ SKYNET-10 
https://www.youtube.com/watch?v=zrWmjCPV1bM
本システムの最新版 SKYNET 10 は、従来のシステム検証バージョンを基盤として正式にリリースされた安定型モデルです。初期のプロトタイプと比較し、SKYNET 10 は複数の重要な技術的進化と最適化を遂げました。

大規模言語モデル（LLM）のローカル化
　従来使用していた ChatGPT のクラウドAPIを廃止し、軽量なローカルLLMモデルへと移行しました。これに加えて、特定領域に特化したプロンプト設計を行い、システム全体の安定性と応答速度を大幅に向上させるとともに、プライバシー保護の観点でも優れた性能を実現しています。

音声認識（ASR）および音声合成（TTS）のローカル化
　外部APIへの依存を完全に排除し、音声認識と音声合成の処理をローカルで完結する構成としました。これにより、多言語対応や柔軟な音声拡張が可能となり、リアルタイム性と制御自由度が向上しました。

独自開発のユーザーインターフェース（UI）
　操作性とユーザーエクスペリエンスの強化を目的に、UIは完全自社開発で構築されており、複数の設定項目をリアルタイムで操作できる柔軟なインターフェースを提供します。

上位機・下位機による分散制御構成
　本システムは「上位機（マスター）+ 下位機（スレーブ）」という分散アーキテクチャを採用しています。
　上位機では、自然言語理解・音声処理・システム統括に加えて、YOLOベースの画像認識処理も担当しており、ユーザーの発話と視覚情報を統合したインテリジェントな判断が可能です。
　一方、下位機は Raspberry Pi 上で動作し、ロボットアームの運動制御、逆運動学の演算、アクチュエータへの指令実行をリアルタイムで処理します。この構成により、システムは高い応答性とモジュールごとの独立性を両立させています。




---

# 注意（Notice）  
本リポジトリは、現在特許出願準備中の技術に関するポートフォリオの一部です。
そのため、ソースコードの全ては公開しておりません。
共同研究・導入検討などのご相談は、お気軽にご連絡ください。


